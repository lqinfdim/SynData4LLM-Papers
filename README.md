# SynData-Papers
Paper list of synthesizing data for LLM.



### Survey Papers

- Tan Z, Beigi A, Wang S, et al. Large language models for data annotation: A survey[J]. arXiv preprint arXiv:2402.13446, 2024.
  
  [Paper](https://arxiv.org/abs/2402.13446) [Project](https://github.com/zhen-tan-dmml/llm4annotation)
  


### Research Papers

- Wu S, Huang Y, Gao C, et al. UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models[J]. arXiv preprint arXiv:2406.18966, 2024.

  [Paper](https://arxiv.org/abs/2406.18966)

- Dong Q, Dong L, Zhang X, et al. Self-Boosting Large Language Models with Synthetic Preference Data[J]. arXiv preprint arXiv:2410.06961, 2024.

  [Paper](https://arxiv.org/abs/2410.06961)

- Lupidi A, Gemmell C, Cancedda N, et al. Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources[J]. arXiv preprint arXiv:2409.08239, 2024.

  [Paper](https://arxiv.org/abs/2409.08239)

- Yang Z, Band N, Li S, et al. Synthetic continued pretraining[J]. arXiv preprint arXiv:2409.07431, 2024.

  [Paper](https://arxiv.org/abs/2409.07431)
  [Project](https://github.com/zitongyang/synthetic_continued_pretraining)
  
- Liu R, Wei J, Liu F, et al. Best practices and lessons learned on synthetic data for language models[J]. arXiv preprint arXiv:2404.07503, 2024.

  [Paper](https://arxiv.org/abs/2404.07503)
  
- Mitra A, Del Corro L, Zheng G, et al. Agentinstruct: Toward generative teaching with agentic flows[J]. arXiv preprint arXiv:2407.03502, 2024.

  [Paper](https://arxiv.org/abs/2407.03502)


- Zhu H, Su J, Lun T, et al. FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs Only[J]. arXiv preprint arXiv:2408.01323, 2024.

  [Paper](https://arxiv.org/abs/2408.01323)

- Li H, Dong Q, Tang Z, et al. Synthetic data (almost) from scratch: Generalized instruction tuning for language models[J]. arXiv preprint arXiv:2402.13064, 2024.

  [Paper](https://arxiv.org/abs/2402.13064)



#### SynData For Pre-training

- Yang Z, Band N, Li S, et al. Synthetic continued pretraining[J]. arXiv preprint arXiv:2409.07431, 2024.
  [Paper](https://arxiv.org/abs/2409.07431)

- Abdin M, Aneja J, Behl H, et al. Phi-4 technical report[J]. arXiv preprint arXiv:2412.08905, 2024.
  [Paper](https://arxiv.org/abs/2412.08905)

- Maini P, Seto S, Bai H, et al. Rephrasing the web: A recipe for compute and data-efficient language modeling[J]. arXiv preprint arXiv:2401.16380, 2024.
  [Paper](https://arxiv.org/abs/2401.16380)

- Longpre S, Yauney G, Reif E, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity[J]. arXiv preprint arXiv:2305.13169, 2023.
  [Paper](https://arxiv.org/pdf/2305.13169)

#### SynData For Post-training

- Huang Y, Liu X, Gong Y, et al. Key-point-driven data synthesis with its enhancement on mathematical reasoning[J]. arXiv preprint arXiv:2403.02333, 2024.
  [Paper](https://arxiv.org/abs/2403.02333)


#### SynData For Research

- Ge T, Chan X, Wang X, et al. Scaling synthetic data creation with 1,000,000,000 personas[J]. arXiv preprint arXiv:2406.20094, 2024.
  [Paper](https://arxiv.org/pdf/2406.20094?)


### Analysis Papers

- Gan Z, Liu Y. Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective[J]. arXiv preprint arXiv:2410.01720, 2024.

  [Paper](https://arxiv.org/abs/2410.01720)

- Jiang C, Chan C, Xue W, et al. Importance Weighting Can Help Large Language Models Self-Improve[J]. arXiv preprint arXiv:2408.09849, 2024.

  [Paper](https://arxiv.org/abs/2408.09849)


- Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 177â€“198, Bangkok, Thailand. Association for Computational Linguistics.

  [Paper](https://aclanthology.org/2024.acl-long.12/)

- Bansal H, Hosseini A, Agarwal R, et al. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling[J]. arXiv preprint arXiv:2408.16737, 2024.

  [Paper](https://arxiv.org/abs/2408.16737)

#### SynData Quality Analysis

- Chen H, Waheed A, Li X, et al. On the Diversity of Synthetic Data and its Impact on Training Large Language Models[J]. arXiv preprint arXiv:2410.15226, 2024.
  [Paper](https://arxiv.org/abs/2410.15226)

- Du J, Zhang X, Hu J, et al. Diversity-driven synthesis: Enhancing dataset distillation through directed weight adjustment[J]. arXiv preprint arXiv:2409.17612, 2024.

  [Paper](https://arxiv.org/pdf/2409.17612)
  

#### Model Collapse & Cons of SynData

- Seddik M E A, Chen S W, Hayou S, et al. How bad is training on synthetic data? a statistical analysis of language model collapse[J]. arXiv preprint arXiv:2404.05090, 2024.

  [Paper](https://arxiv.org/abs/2404.05090)

- Dohmatob E, Feng Y, Subramonian A, et al. Strong model collapse[J]. arXiv preprint arXiv:2410.04840, 2024.

  [Paper](https://arxiv.org/pdf/2410.04840)

- Shumailov I, Shumaylov Z, Zhao Y, et al. AI models collapse when trained on recursively generated data[J]. Nature, 2024, 631(8022): 755-759.

  [Paper](https://www.nature.com/articles/s41586-024-07566-y)

- Seddik M E A, Chen S W, Hayou S, et al. How bad is training on synthetic data? a statistical analysis of language model collapse[J]. arXiv preprint arXiv:2404.05090, 2024.

  [Paper](https://arxiv.org/pdf/2404.05090)

- Zhu X, Cheng D, Li H, et al. How to Synthesize Text Data without Model Collapse?[J]. arXiv preprint arXiv:2412.14689, 2024.

  [Paper](https://arxiv.org/abs/2412.14689)




  ### Application

  - Xiong, Z., Papageorgiou, V., Lee, K., & Papailiopoulos, D. (2024). From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data. arXiv preprint arXiv:2406.19292.
  - 
    [Paper](https://arxiv.org/abs/2406.19292)

