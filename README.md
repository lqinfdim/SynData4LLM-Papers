# SynData-Papers
Paper list of synthesizing data for LLM.



### Survey Papers

- Tan Z, Beigi A, Wang S, et al. Large language models for data annotation: A survey[J]. arXiv preprint arXiv:2402.13446, 2024.
  
  [Paper](https://arxiv.org/abs/2402.13446) [Project](https://github.com/zhen-tan-dmml/llm4annotation)
  


### Research Papers

- Wu S, Huang Y, Gao C, et al. UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models[J]. arXiv preprint arXiv:2406.18966, 2024.

  [Paper](https://arxiv.org/abs/2406.18966)

- Dong Q, Dong L, Zhang X, et al. Self-Boosting Large Language Models with Synthetic Preference Data[J]. arXiv preprint arXiv:2410.06961, 2024.

  [Paper](https://arxiv.org/abs/2410.06961)

- Lupidi A, Gemmell C, Cancedda N, et al. Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources[J]. arXiv preprint arXiv:2409.08239, 2024.

  [Paper](https://arxiv.org/abs/2409.08239)

- Yang Z, Band N, Li S, et al. Synthetic continued pretraining[J]. arXiv preprint arXiv:2409.07431, 2024.

  [Paper](https://arxiv.org/abs/2409.07431)
  [Project](https://github.com/zitongyang/synthetic_continued_pretraining)
  
- Liu R, Wei J, Liu F, et al. Best practices and lessons learned on synthetic data for language models[J]. arXiv preprint arXiv:2404.07503, 2024.

  [Paper](https://arxiv.org/abs/2404.07503)
  
- Mitra A, Del Corro L, Zheng G, et al. Agentinstruct: Toward generative teaching with agentic flows[J]. arXiv preprint arXiv:2407.03502, 2024.

  [Paper](https://arxiv.org/abs/2407.03502)


- Zhu H, Su J, Lun T, et al. FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs Only[J]. arXiv preprint arXiv:2408.01323, 2024.

  [Paper](https://arxiv.org/abs/2408.01323)

- Li H, Dong Q, Tang Z, et al. Synthetic data (almost) from scratch: Generalized instruction tuning for language models[J]. arXiv preprint arXiv:2402.13064, 2024.

  [Paper](https://arxiv.org/abs/2402.13064)

### Analysis Papers

- Gan Z, Liu Y. Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective[J]. arXiv preprint arXiv:2410.01720, 2024.

  [Paper](https://arxiv.org/abs/2410.01720)

- Jiang C, Chan C, Xue W, et al. Importance Weighting Can Help Large Language Models Self-Improve[J]. arXiv preprint arXiv:2408.09849, 2024.

  [Paper](https://arxiv.org/abs/2408.09849)


- Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 177â€“198, Bangkok, Thailand. Association for Computational Linguistics.

  [Paper](https://aclanthology.org/2024.acl-long.12/)

- Bansal H, Hosseini A, Agarwal R, et al. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling[J]. arXiv preprint arXiv:2408.16737, 2024.

  [Paper](https://arxiv.org/abs/2408.16737)


- Seddik M E A, Chen S W, Hayou S, et al. How bad is training on synthetic data? a statistical analysis of language model collapse[J]. arXiv preprint arXiv:2404.05090, 2024.

  [Paper](https://arxiv.org/abs/2404.05090)
