# SynData-Papers
Paper list of synthesizing data for LLM.



### Survey Papers

- Tan Z, Beigi A, Wang S, et al. Large language models for data annotation: A survey[J]. arXiv preprint arXiv:2402.13446, 2024.
  
  [Paper](https://arxiv.org/abs/2402.13446)

  [Project](https://github.com/zhen-tan-dmml/llm4annotation)
  


### Research Papers

- Wu S, Huang Y, Gao C, et al. UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models[J]. arXiv preprint arXiv:2406.18966, 2024.

  [Paper](https://arxiv.org/abs/2406.18966)

- Dong Q, Dong L, Zhang X, et al. Self-Boosting Large Language Models with Synthetic Preference Data[J]. arXiv preprint arXiv:2410.06961, 2024.

  [Paper](https://arxiv.org/abs/2410.06961)

- Lupidi A, Gemmell C, Cancedda N, et al. Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources[J]. arXiv preprint arXiv:2409.08239, 2024.

  [Paper](https://arxiv.org/abs/2409.08239)

- Yang Z, Band N, Li S, et al. Synthetic continued pretraining[J]. arXiv preprint arXiv:2409.07431, 2024.

  [Paper](https://arxiv.org/abs/2409.07431)
  [Project](https://github.com/zitongyang/synthetic_continued_pretraining)



### Analysis Papers

- Gan Z, Liu Y. Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective[J]. arXiv preprint arXiv:2410.01720, 2024.

  [Paper](https://arxiv.org/abs/2410.01720)

- Jiang C, Chan C, Xue W, et al. Importance Weighting Can Help Large Language Models Self-Improve[J]. arXiv preprint arXiv:2408.09849, 2024.

  [Paper](https://arxiv.org/abs/2408.09849)


- Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 177â€“198, Bangkok, Thailand. Association for Computational Linguistics.

  [Paper](https://aclanthology.org/2024.acl-long.12/)

  
